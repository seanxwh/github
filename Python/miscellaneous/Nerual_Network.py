#A neural network implementation for data/labels classification using Python. User can train the network
#by provide data that has labels in it. User can also fine tune the network by modifying it parameters (e.ghidden layers, learning rate, etc)

# Package imports
import matplotlib.pyplot as plt
import math
import numpy as np
import time

# a test case that using data that generated by numpy, one can also use other dataset with its own
#labels (e.g MNIST http://yann.lecun.com/exdb/mnist/)

#data generation, and plot the data
N = 300 # number of points per class
D = 2 # dimensionality
K = 2 # number of classes
X = np.zeros((N*K,D)) # data matrix (each row = single example)
y = np.zeros(N*K, dtype='uint8') # class labels
for j in xrange(K):
    ix = range(N*j,N*(j+1))
    r = np.linspace(0.0,1,N) # radius
    t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta
    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]
    y[ix] = j
plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)

# a function that plot the decision boundary (i.e which label/class a data belong to),
def plot_decision_boundary(pred_func):
    # Set min and max values and give it some padding
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    print np.c_[xx.ravel(), yy.ravel()][0]
    # Predict the function value for the whole gid
    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])
    print Z
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)

# the neural net implementation
def forwardPropagation(reg,numLyr,X,y,w,b):
    a=[]
    a.append(X)
    numEmp = X.shape[0]
    totalRegLoss=0
    for i in range(numLyr):
        a.append(np.maximum(0,(np.dot(a[i],w[i])+b[i])))#ReLu activation
        totalRegLoss += 0.5*reg*np.sum(w[i]*w[i])
    fnOutput = np.exp(a[numLyr])
    fnOutputProb = fnOutput/np.sum(fnOutput,axis=1, keepdims=True)
    crtOutputProb = -np.log(fnOutputProb[range(numEmp),y])
    dataLoss = np.sum(crtOutputProb)/numEmp
    costFunc = dataLoss+totalRegLoss
    return a,fnOutput,fnOutputProb,crtOutputProb,costFunc

def predict(reg,numLyr,X,w,b):
    a=[]
    a.append(X)
    numEmp = X.shape[0]
    totalRegLoss=0
    for i in range(numLyr):
        a.append(np.maximum(0,(np.dot(a[i],w[i])+b[i])))#ReLu activation, also the a[numlyer] is the final activation layer(AL) for each class, the
                                                        # AL of a class over the sum of all the classes is the softmax repesentation
        totalRegLoss += 0.5*reg*np.sum(w[i]*w[i])
    fnOutput = np.exp(a[numLyr])
    fnOutputProb = fnOutput/np.sum(fnOutput,axis=1, keepdims=True)
    predicted_class = np.argmax(a[numLyr], axis=1)
    return predicted_class


def add(x,y):
    return x+y

def backPropagation(lrnRate,reg,numLyr,probs,a,X,y,w,b):
    numEmp = X.shape[0]
    dltPrd = probs
    dltPrd[range(numEmp),y]-=1 # result of the softmax derivertive
    dltPrd /= numEmp
    da=[]
    dw=[]
    db=[]
    for i in reversed(range(numLyr)):
        if(i == numLyr-1):
            da.insert(0,np.dot(dltPrd,w[i].T))
            da[0][a[i] <= 0]=0
            dw.insert(0,np.dot(a[i].T, dltPrd))
            db.insert(0,np.sum(dltPrd,axis=0,keepdims=True))
        elif(i == 0):
            dw.insert(0,np.dot(X.T, da[i])) #the da[i] has the value of 'da[i+1] (e.g: the da from prev layer toward ouput)' if we have the whole complete list,
                                            #in this case, we current don't have value for da[i+1] of the
                                            #complete list, because the first element of the list is d[i]. same idea apply to
                                            #the index cal of da, db & dw
            db.insert(0,np.sum(da[i],axis=0,keepdims=True))

        else:
            da.insert(0,np.dot(da[0],w[i].T))
            da[0][a[i]<=0]=0
            dw.insert(0,np.dot(a[i].T, da[0]))
            db.insert(0,np.sum(da[0],axis=0,keepdims=True))
    regDW = [(reg)*i for i in w]
    dw = map(add,dw,regDW)
    lrnDW = [-lrnRate*i for i in dw]
    w = map(add,w,lrnDW)
    lrnDB=[-(lrnRate)*i for i in db]
    b= map(add,b,lrnDB)
    return w,b


def model(X,y,hdim=200,numLyr=3,iterations=2500,step_size=3e-2,reg=7e-5):
#     start = time.clock()
    K = np.unique(y).size#number of classes from the output labels
    xEmp = X.shape[0]
    n=xEmp
    xDim = X.shape[1]
    w=[]
    b=[]
    for i in xrange(numLyr):
        if(i==numLyr-1):
            w.append(0.01 * np.random.randn(hdim,K))
            b.append(np.zeros((1,K)))
        elif(i==0):
            w.append(0.01 * np.random.randn(xDim,hdim))
            b.append(np.zeros((1,hdim)))
        else:
            w.append(0.01 * np.random.randn(hdim,hdim))
            b.append(np.zeros((1,hdim)))
    for t in range(iterations):
        a,fnOutput,fnOutputProb,crtOutputProb,costFunc = forwardPropagation(reg, numLyr, X, y, w, b)
        w, b = backPropagation(step_size, reg, numLyr, fnOutputProb, a, X, y, w, b)
        if (t % 100 == 0):
            print "iteration %d: loss %f" % (t, costFunc)
        if (t==iterations-1):
            predicted_class = np.argmax(a[numLyr], axis=1)
            print 'training accuracy: %.2f' % (np.mean(predicted_class == y))
            print 'end\n'
#     end=time.clock()
#     time=end-start
#     print "time spend:", time
    return reg,numLyr,w,b

#testing and plot the decision boundary
reg,numLyr,w,b=model(X,y,200,3,2500,.3,7e-5)
plot_decision_boundary(lambda k: predict(reg,numLyr,k,w,b))
